--- linux-2.6.25.16/arch/powerpc/lib/memcpy_64.S.orig	2008-08-20 18:16:13.000000000 +0000
+++ linux-2.6.25.16/arch/powerpc/lib/memcpy_64.S	2008-08-22 20:57:08.000000000 +0000
@@ -1,173 +1,229 @@
-/*
- * Copyright (C) 2002 Paul Mackerras, IBM Corp.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
- */
-#include <asm/processor.h>
-#include <asm/ppc_asm.h>
-
-	.align	7
-_GLOBAL(memcpy)
-	std	r3,48(r1)	/* save destination pointer for return value */
-	PPC_MTOCRF	0x01,r5
-	cmpldi	cr1,r5,16
-	neg	r6,r3		# LS 3 bits = # bytes to 8-byte dest bdry
-	andi.	r6,r6,7
-	dcbt	0,r4
-	blt	cr1,.Lshort_copy
-	bne	.Ldst_unaligned
-.Ldst_aligned:
-	andi.	r0,r4,7
-	addi	r3,r3,-16
-	bne	.Lsrc_unaligned
-	srdi	r7,r5,4
-	ld	r9,0(r4)
-	addi	r4,r4,-8
-	mtctr	r7
-	andi.	r5,r5,7
-	bf	cr7*4+0,2f
-	addi	r3,r3,8
-	addi	r4,r4,8
-	mr	r8,r9
-	blt	cr1,3f
-1:	ld	r9,8(r4)
-	std	r8,8(r3)
-2:	ldu	r8,16(r4)
-	stdu	r9,16(r3)
-	bdnz	1b
-3:	std	r8,8(r3)
-	beq	3f
-	addi	r3,r3,16
-	ld	r9,8(r4)
-.Ldo_tail:
-	bf	cr7*4+1,1f
-	rotldi	r9,r9,32
-	stw	r9,0(r3)
-	addi	r3,r3,4
-1:	bf	cr7*4+2,2f
-	rotldi	r9,r9,16
-	sth	r9,0(r3)
-	addi	r3,r3,2
-2:	bf	cr7*4+3,3f
-	rotldi	r9,r9,8
-	stb	r9,0(r3)
-3:	ld	r3,48(r1)	/* return dest pointer */
-	blr
-
-.Lsrc_unaligned:
-	srdi	r6,r5,3
-	addi	r5,r5,-16
-	subf	r4,r0,r4
-	srdi	r7,r5,4
-	sldi	r10,r0,3
-	cmpdi	cr6,r6,3
-	andi.	r5,r5,7
-	mtctr	r7
-	subfic	r11,r10,64
-	add	r5,r5,r0
-
-	bt	cr7*4+0,0f
-
-	ld	r9,0(r4)	# 3+2n loads, 2+2n stores
-	ld	r0,8(r4)
-	sld	r6,r9,r10
-	ldu	r9,16(r4)
-	srd	r7,r0,r11
-	sld	r8,r0,r10
-	or	r7,r7,r6
-	blt	cr6,4f
-	ld	r0,8(r4)
-	# s1<< in r8, d0=(s0<<|s1>>) in r7, s3 in r0, s2 in r9, nix in r6 & r12
-	b	2f
-
-0:	ld	r0,0(r4)	# 4+2n loads, 3+2n stores
-	ldu	r9,8(r4)
-	sld	r8,r0,r10
-	addi	r3,r3,-8
-	blt	cr6,5f
-	ld	r0,8(r4)
-	srd	r12,r9,r11
-	sld	r6,r9,r10
-	ldu	r9,16(r4)
-	or	r12,r8,r12
-	srd	r7,r0,r11
-	sld	r8,r0,r10
-	addi	r3,r3,16
-	beq	cr6,3f
-
-	# d0=(s0<<|s1>>) in r12, s1<< in r6, s2>> in r7, s2<< in r8, s3 in r9
-1:	or	r7,r7,r6
-	ld	r0,8(r4)
-	std	r12,8(r3)
-2:	srd	r12,r9,r11
-	sld	r6,r9,r10
-	ldu	r9,16(r4)
-	or	r12,r8,r12
-	stdu	r7,16(r3)
-	srd	r7,r0,r11
-	sld	r8,r0,r10
-	bdnz	1b
-
-3:	std	r12,8(r3)
-	or	r7,r7,r6
-4:	std	r7,16(r3)
-5:	srd	r12,r9,r11
-	or	r12,r8,r12
-	std	r12,24(r3)
-	beq	4f
-	cmpwi	cr1,r5,8
-	addi	r3,r3,32
-	sld	r9,r9,r10
-	ble	cr1,.Ldo_tail
-	ld	r0,8(r4)
-	srd	r7,r0,r11
-	or	r9,r7,r9
-	b	.Ldo_tail
-
-.Ldst_unaligned:
-	PPC_MTOCRF	0x01,r6		# put #bytes to 8B bdry into cr7
-	subf	r5,r6,r5
-	li	r7,0
-	cmpldi	r1,r5,16
-	bf	cr7*4+3,1f
-	lbz	r0,0(r4)
-	stb	r0,0(r3)
-	addi	r7,r7,1
-1:	bf	cr7*4+2,2f
-	lhzx	r0,r7,r4
-	sthx	r0,r7,r3
-	addi	r7,r7,2
-2:	bf	cr7*4+1,3f
-	lwzx	r0,r7,r4
-	stwx	r0,r7,r3
-3:	PPC_MTOCRF	0x01,r5
-	add	r4,r6,r4
-	add	r3,r6,r3
-	b	.Ldst_aligned
-
-.Lshort_copy:
-	bf	cr7*4+0,1f
-	lwz	r0,0(r4)
-	lwz	r9,4(r4)
-	addi	r4,r4,8
-	stw	r0,0(r3)
-	stw	r9,4(r3)
-	addi	r3,r3,8
-1:	bf	cr7*4+1,2f
-	lwz	r0,0(r4)
-	addi	r4,r4,4
-	stw	r0,0(r3)
-	addi	r3,r3,4
-2:	bf	cr7*4+2,3f
-	lhz	r0,0(r4)
-	addi	r4,r4,2
-	sth	r0,0(r3)
-	addi	r3,r3,2
-3:	bf	cr7*4+3,4f
-	lbz	r0,0(r4)
-	stb	r0,0(r3)
-4:	ld	r3,48(r1)	/* return dest pointer */
-	blr
+/*
+ * Copyright (C) 2008 Gunnar von Boehn, IBM Corp.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ *
+ *
+ * memcpy (and copy_4K_page) routine optimized for CELL-BE-PPC
+ *
+ * The CELL PPC core has 1 integerunit and 1 load/store unit
+ * CELL: 1st level data cache = 32K - 2nd level data cache = 512K
+ * - 3rd level data cache = 0K
+ * To improve copy performance we need to prefetch source data
+ * far ahead to hide this latency
+ * For best performance instruction forms ending in "." like "andi."
+ * should be avoided as they are implemented in microcode on CELL.
+ *
+ * The below code is loop unrolled for the CELL cache line of 128 bytes.
+ */
+
+#include <asm/processor.h>
+#include <asm/ppc_asm.h>
+
+#define PREFETCH_AHEAD 6
+#define ZERO_AHEAD 4
+
+        .align  7
+_GLOBAL(memcpy)
+	dcbt	0,r4		/* Prefetch ONE SRC cacheline */
+	cmpldi	cr1,r5,16	/* is size < 16 ? */
+	mr	r6,r3
+	blt+	cr1,.Lshortcopy
+
+.Lbigcopy:
+	neg	r8,r3		/* LS 3 bits = # bytes to 8-byte dest bdry */
+        clrldi  r8,r8,64-4	/* aling to 16byte boundary */
+	sub     r7,r4,r3
+	cmpldi	cr0,r8,0
+	beq+	.Ldst_aligned
+
+.Ldst_unaligned:
+	mtcrf	0x01,r8		/* put #bytes to boundary into cr7 */
+	subf	r5,r8,r5
+
+	bf	cr7*4+3,1f
+	lbzx	r0,r7,r6	/* copy 1 byte */
+	stb	r0,0(r6)
+	addi	r6,r6,1
+1:	bf	cr7*4+2,2f
+	lhzx	r0,r7,r6	/* copy 2 byte */
+	sth	r0,0(r6)
+	addi	r6,r6,2
+2:	bf	cr7*4+1,4f
+	lwzx	r0,r7,r6	/* copy 4 byte */
+	stw	r0,0(r6)
+	addi	r6,r6,4
+4:	bf	cr7*4+0,8f
+	ldx	r0,r7,r6	/* copy 8 byte */
+	std	r0,0(r6)
+	addi	r6,r6,8
+8:
+	add	r4,r7,r6
+
+.Ldst_aligned:
+
+	cmpdi	cr5,r5,128-1
+
+	neg	r7,r6
+	addi	r6,r6,-8	/* prepare for stdu */
+	addi	r4,r4,-8	/* prepare for ldu */
+
+	clrldi  r7,r7,64-7	/* align to cacheline boundary */
+	ble+	cr5,.Llessthancacheline
+
+
+	cmpldi	cr6,r7,0
+	subf	r5,r7,r5
+	srdi	r7,r7,4		/* divide size by 16 */
+	srdi	r10,r5,7	/* number of cache lines to copy */
+
+
+	cmpldi	r10,0
+	li	r11,0			/* number cachelines to copy with prefetch */
+	beq	.Lnocacheprefetch
+
+	cmpldi	r10,PREFETCH_AHEAD
+	li	r12,128+8		/* prefetch distance*/
+	ble	.Llessthanmaxprefetch
+
+	subi	r11,r10,PREFETCH_AHEAD
+	li	r10,PREFETCH_AHEAD
+.Llessthanmaxprefetch:
+
+	mtctr	r10
+.LprefetchSRC:
+	dcbt    r12,r4
+        addi    r12,r12,128
+        bdnz    .LprefetchSRC
+.Lnocacheprefetch:
+
+
+	mtctr	r7
+	cmpldi	cr1,r5,128
+	clrldi  r5,r5,64-7
+
+	beq	cr6,.Lcachelinealigned	/* 	*/
+.Laligntocacheline:
+	ld 	r9,0x08(r4)
+	ldu	r7,0x10(r4)
+	std	r9,0x08(r6)
+	stdu	r7,0x10(r6)
+	bdnz	.Laligntocacheline
+
+
+.Lcachelinealigned:				/* copy while cache lines */
+
+
+	blt- 	cr1,.Llessthancacheline		/* size <128 */
+
+.Louterloop:
+        cmpdi   r11,0
+	mtctr	r11
+	beq-	.Lendloop
+
+	li	r11,128*ZERO_AHEAD +8		/* DCBZ dist */
+
+.align	4
+	/* Copy whole cachelines, optimized by prefetching SRC cacheline */
+.Lloop: 				/* Copy aligned body */
+	dcbt    r12,r4			/* PREFETCH SOURCE some cache lines ahead*/
+        ld      r9, 0x08(r4)
+	dcbz	r11,r6
+        ld      r7, 0x10(r4)    	/* 4 register stride copy */
+        ld      r8, 0x18(r4)		/* 4 are optimal to hide 1st level cache lantency*/
+        ld      r0, 0x20(r4)
+        std     r9, 0x08(r6)
+        std     r7, 0x10(r6)
+        std     r8, 0x18(r6)
+        std     r0, 0x20(r6)
+        ld      r9, 0x28(r4)
+        ld      r7, 0x30(r4)
+        ld      r8, 0x38(r4)
+        ld      r0, 0x40(r4)
+        std     r9, 0x28(r6)
+        std     r7, 0x30(r6)
+        std     r8, 0x38(r6)
+        std     r0, 0x40(r6)
+        ld      r9, 0x48(r4)
+        ld      r7, 0x50(r4)
+        ld      r8, 0x58(r4)
+        ld      r0, 0x60(r4)
+        std     r9, 0x48(r6)
+        std     r7, 0x50(r6)
+        std     r8, 0x58(r6)
+        std     r0, 0x60(r6)
+        ld      r9, 0x68(r4)
+        ld      r7, 0x70(r4)
+        ld      r8, 0x78(r4)
+        ldu     r0, 0x80(r4)
+        std     r9, 0x68(r6)
+        std     r7, 0x70(r6)
+        std     r8, 0x78(r6)
+        stdu    r0, 0x80(r6)
+
+	bdnz    .Lloop
+.Lendloop:
+
+
+        cmpdi   r10,0
+	sldi    r10,r10,2         	/* adjust from 128 to 32 byte stride */
+        beq-     .Lendloop2
+        mtctr 	r10
+.Lloop2: 				/* Copy aligned body */
+        ld      r9, 0x08(r4)
+        ld      r7, 0x10(r4)
+        ld      r8, 0x18(r4)
+        ldu     r0, 0x20(r4)
+        std     r9, 0x08(r6)
+        std     r7, 0x10(r6)
+        std     r8, 0x18(r6)
+        stdu    r0, 0x20(r6)
+
+	bdnz    .Lloop2
+
+.Lendloop2:
+
+
+.Llessthancacheline:		/* less than cache to do ? */
+	cmpldi	cr0,r5,16
+	srdi	r7,r5,4		/* divide size by 16 */
+        blt-    .Ldo_lt16
+	mtctr	r7
+.Lcopy_remaining:
+	ld 	r8,0x08(r4)
+	ldu	r7,0x10(r4)
+	std	r8,0x08(r6)
+	stdu	r7,0x10(r6)
+	bdnz	.Lcopy_remaining
+
+
+.Ldo_lt16:			/* less than 16 ? */
+	cmpldi	cr0,r5,0	/* copy remaining bytes (0-15) */
+	beqlr+			/* no rest to copy */
+	addi	r4,r4,8
+	addi	r6,r6,8
+.Lshortcopy:			/* SIMPLE COPY to handle size =< 15 bytes */
+	mtcrf	0x01,r5
+	sub     r7,r4,r6
+	bf-	cr7*4+0,8f
+	ldx	r0,r7,r6	/* copy 8 byte */
+	std	r0,0(r6)
+	addi	r6,r6,8
+8:
+	bf	cr7*4+1,4f
+	lwzx	r0,r7,r6	/* copy 4 byte */
+	stw	r0,0(r6)
+	addi	r6,r6,4
+4:
+	bf	cr7*4+2,2f
+	lhzx	r0,r7,r6	/* copy 2 byte */
+	sth	r0,0(r6)
+	addi	r6,r6,2
+2:
+	bf	cr7*4+3,1f
+	lbzx	r0,r7,r6	/* copy 1 byte */
+	stb	r0,0(r6)
+1:	blr
\ No newline at end of file
